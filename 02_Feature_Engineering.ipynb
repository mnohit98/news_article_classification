{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# News Article Classification - Part 2: Feature Engineering\n",
        "\n",
        "## Overview\n",
        "This notebook covers:\n",
        "1. Loading preprocessed data\n",
        "2. Extracting textual features (word count, character count, etc.)\n",
        "3. TF-IDF Vectorization\n",
        "4. Word2Vec Embeddings (optional)\n",
        "5. Preparing features for model training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "# Feature extraction\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# Utilities\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Load Preprocessed Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load preprocessed data\n",
        "df = pd.read_csv('data/processed_articles.csv')\n",
        "\n",
        "print(f\"Dataset loaded successfully!\")\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"First few rows:\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Extract Textual Features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract various textual features\n",
        "def extract_text_features(df):\n",
        "    \"\"\"Extract textual features from reviews\"\"\"\n",
        "    features = df.copy()\n",
        "    \n",
        "    # Basic length features\n",
        "    features['char_count'] = features['cleaned_text'].str.len()\n",
        "    features['word_count'] = features['cleaned_text'].str.split().str.len()\n",
        "    features['sentence_count'] = features['cleaned_text'].str.split('.').str.len()\n",
        "    \n",
        "    # Average word length\n",
        "    features['avg_word_length'] = features['char_count'] / (features['word_count'] + 1)\n",
        "    \n",
        "    # Count of uppercase letters (if any remain after preprocessing)\n",
        "    features['uppercase_count'] = features['cleaned_text'].str.findall(r'[A-Z]').str.len()\n",
        "    \n",
        "    # Count of digits (if any remain)\n",
        "    features['digit_count'] = features['cleaned_text'].str.findall(r'\\d').str.len()\n",
        "    \n",
        "    # Count of special characters\n",
        "    features['special_char_count'] = features['cleaned_text'].str.findall(r'[^a-zA-Z0-9\\s]').str.len()\n",
        "    \n",
        "    # Count of exclamation marks and question marks (category indicators)\n",
        "    features['exclamation_count'] = features['cleaned_text'].str.count('!')\n",
        "    features['question_count'] = features['cleaned_text'].str.count('?')\n",
        "    \n",
        "    return features\n",
        "\n",
        "# Extract features\n",
        "df_features = extract_text_features(df)\n",
        "\n",
        "print(\"Textual features extracted!\")\n",
        "print(\"\\nFeature Statistics:\")\n",
        "print(df_features[['char_count', 'word_count', 'avg_word_length']].describe())\n",
        "print(\"\\nSample features:\")\n",
        "df_features[['cleaned_text', 'char_count', 'word_count', 'avg_word_length']].head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: TF-IDF Vectorization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize TF-IDF Vectorizer\n",
        "# Using common parameters for text classification\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    max_features=5000,  # Top 5000 features\n",
        "    ngram_range=(1, 2),  # Unigrams and bigrams\n",
        "    min_df=2,  # Minimum document frequency\n",
        "    max_df=0.95,  # Maximum document frequency (ignore very common words)\n",
        "    sublinear_tf=True  # Apply sublinear tf scaling\n",
        ")\n",
        "\n",
        "# Fit and transform the cleaned reviews\n",
        "print(\"Fitting TF-IDF vectorizer...\")\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(df_features['cleaned_text'])\n",
        "\n",
        "print(f\"TF-IDF matrix shape: {X_tfidf.shape}\")\n",
        "print(f\"Number of features: {X_tfidf.shape[1]}\")\n",
        "\n",
        "# Get feature names\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "print(f\"\\nSample feature names (first 20):\")\n",
        "print(feature_names[:20])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Word2Vec Embeddings (Optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare tokenized sentences for Word2Vec\n",
        "tokenized_reviews = [review.split() for review in df_features['cleaned_text']]\n",
        "\n",
        "# Train Word2Vec model\n",
        "print(\"Training Word2Vec model...\")\n",
        "word2vec_model = Word2Vec(\n",
        "    sentences=tokenized_reviews,\n",
        "    vector_size=100,  # Dimension of word vectors\n",
        "    window=5,  # Context window size\n",
        "    min_count=2,  # Minimum word frequency\n",
        "    workers=4,  # Number of threads\n",
        "    sg=0  # 0 for CBOW, 1 for Skip-gram\n",
        ")\n",
        "\n",
        "print(f\"Word2Vec model trained!\")\n",
        "print(f\"Vocabulary size: {len(word2vec_model.wv.key_to_index)}\")\n",
        "\n",
        "# Create document vectors by averaging word vectors\n",
        "def get_document_vector(words, model):\n",
        "    \"\"\"Get document vector by averaging word vectors\"\"\"\n",
        "    vectors = [model.wv[word] for word in words if word in model.wv]\n",
        "    if len(vectors) > 0:\n",
        "        return np.mean(vectors, axis=0)\n",
        "    else:\n",
        "        return np.zeros(model.vector_size)\n",
        "\n",
        "# Create document vectors\n",
        "print(\"Creating document vectors...\")\n",
        "X_word2vec = np.array([get_document_vector(review, word2vec_model) for review in tokenized_reviews])\n",
        "\n",
        "print(f\"Word2Vec matrix shape: {X_word2vec.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Combine Features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract textual features as numpy array\n",
        "textual_features = df_features[['char_count', 'word_count', 'avg_word_length', \n",
        "                                 'exclamation_count', 'question_count']].values\n",
        "\n",
        "# Normalize textual features\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "textual_features_scaled = scaler.fit_transform(textual_features)\n",
        "\n",
        "print(f\"Textual features shape: {textual_features_scaled.shape}\")\n",
        "\n",
        "# Option 1: Combine TF-IDF with textual features\n",
        "from scipy.sparse import hstack\n",
        "X_combined_tfidf = hstack([X_tfidf, textual_features_scaled])\n",
        "\n",
        "print(f\"Combined TF-IDF + Textual features shape: {X_combined_tfidf.shape}\")\n",
        "\n",
        "# Option 2: Combine Word2Vec with textual features\n",
        "X_combined_word2vec = np.hstack([X_word2vec, textual_features_scaled])\n",
        "\n",
        "print(f\"Combined Word2Vec + Textual features shape: {X_combined_word2vec.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Prepare Target Variable\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Encode target variable\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(df_features['category'])\n",
        "\n",
        "print(f\"Target variable shape: {y.shape}\")\n",
        "print(f\"Class distribution:\")\n",
        "print(pd.Series(y).value_counts())\n",
        "print(f\"\\nLabel mapping:\")\n",
        "for i, label in enumerate(label_encoder.classes_):\n",
        "    print(f\"  {i}: {label}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Split Data into Train and Test Sets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data for TF-IDF features\n",
        "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(\n",
        "    X_combined_tfidf, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Split data for Word2Vec features\n",
        "X_train_word2vec, X_test_word2vec, _, _ = train_test_split(\n",
        "    X_combined_word2vec, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(\"Data split completed!\")\n",
        "print(f\"Training set size (TF-IDF): {X_train_tfidf.shape}\")\n",
        "print(f\"Test set size (TF-IDF): {X_test_tfidf.shape}\")\n",
        "print(f\"Training labels: {y_train.shape}\")\n",
        "print(f\"Test labels: {y_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Save Features and Vectorizers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create models directory\n",
        "os.makedirs('models', exist_ok=True)\n",
        "\n",
        "# Save TF-IDF vectorizer\n",
        "with open('models/tfidf_vectorizer.pkl', 'wb') as f:\n",
        "    pickle.dump(tfidf_vectorizer, f)\n",
        "\n",
        "# Save Word2Vec model\n",
        "word2vec_model.save('models/word2vec_model.model')\n",
        "\n",
        "# Save scaler\n",
        "with open('models/feature_scaler.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler, f)\n",
        "\n",
        "# Save label encoder\n",
        "with open('models/label_encoder.pkl', 'wb') as f:\n",
        "    pickle.dump(label_encoder, f)\n",
        "\n",
        "# Save processed features\n",
        "np.savez('data/features.npz',\n",
        "         X_train_tfidf=X_train_tfidf,\n",
        "         X_test_tfidf=X_test_tfidf,\n",
        "         X_train_word2vec=X_train_word2vec,\n",
        "         X_test_word2vec=X_test_word2vec,\n",
        "         y_train=y_train,\n",
        "         y_test=y_test)\n",
        "\n",
        "print(\"All features and vectorizers saved successfully!\")\n",
        "print(\"Files saved:\")\n",
        "print(\"- models/tfidf_vectorizer.pkl\")\n",
        "print(\"- models/word2vec_model.model\")\n",
        "print(\"- models/feature_scaler.pkl\")\n",
        "print(\"- models/label_encoder.pkl\")\n",
        "print(\"- data/features.npz\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### Key Accomplishments:\n",
        "1. ✅ Extracted textual features (word count, character count, etc.)\n",
        "2. ✅ Created TF-IDF vectors with 5000 features\n",
        "3. ✅ Generated Word2Vec embeddings (100 dimensions)\n",
        "4. ✅ Combined features for model training\n",
        "5. ✅ Split data into train and test sets\n",
        "6. ✅ Saved all vectorizers and features\n",
        "\n",
        "### Next Steps:\n",
        "- Proceed to Model Development notebook\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
