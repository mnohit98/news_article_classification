{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# News Article Classification - Part 5: Predict New Articles\n",
        "\n",
        "## Overview\n",
        "This notebook demonstrates how to use the trained model to predict categories for new news articles.\n",
        "\n",
        "## Steps:\n",
        "1. Load trained model and vectorizers\n",
        "2. Preprocess new article text\n",
        "3. Make category predictions\n",
        "4. Get prediction probabilities for all categories\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Import Libraries and Load Preprocessing Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import re\n",
        "\n",
        "# Text processing\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download NLTK data if needed\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "except LookupError:\n",
        "    nltk.download('wordnet')\n",
        "\n",
        "# Initialize text preprocessing components\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Clean text by removing HTML tags, special characters, and extra whitespace\"\"\"\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    \n",
        "    text = str(text)\n",
        "    text = re.sub(r'<[^>]+>', '', text)  # Remove HTML tags\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)  # Remove URLs\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra whitespace\n",
        "    return text\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Complete text preprocessing pipeline\"\"\"\n",
        "    text = clean_text(text)\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "print(\"Libraries and preprocessing functions loaded!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Load Trained Model and Vectorizers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load TF-IDF vectorizer\n",
        "with open('models/tfidf_vectorizer.pkl', 'rb') as f:\n",
        "    tfidf_vectorizer = pickle.load(f)\n",
        "\n",
        "# Load feature scaler\n",
        "with open('models/feature_scaler.pkl', 'rb') as f:\n",
        "    feature_scaler = pickle.load(f)\n",
        "\n",
        "# Load label encoder\n",
        "with open('models/label_encoder.pkl', 'rb') as f:\n",
        "    label_encoder = pickle.load(f)\n",
        "\n",
        "# Load best model\n",
        "with open('models/best_model.pkl', 'rb') as f:\n",
        "    model = pickle.load(f)\n",
        "\n",
        "print(\"Model and vectorizers loaded successfully!\")\n",
        "print(f\"Model type: {type(model).__name__}\")\n",
        "print(f\"Label classes: {label_encoder.classes_}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Prediction Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_category(article_text):\n",
        "    \"\"\"\n",
        "    Predict category for a given article text\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    article_text : str\n",
        "        The news article text to analyze\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    dict : Dictionary containing prediction, probability, and confidence\n",
        "    \"\"\"\n",
        "    # Preprocess the text\n",
        "    cleaned_text = preprocess_text(article_text)\n",
        "    \n",
        "    # Extract textual features\n",
        "    char_count = len(cleaned_text)\n",
        "    word_count = len(cleaned_text.split())\n",
        "    avg_word_length = char_count / (word_count + 1) if word_count > 0 else 0\n",
        "    exclamation_count = cleaned_text.count('!')\n",
        "    question_count = cleaned_text.count('?')\n",
        "    \n",
        "    # Transform text using TF-IDF\n",
        "    text_tfidf = tfidf_vectorizer.transform([cleaned_text])\n",
        "    \n",
        "    # Scale textual features\n",
        "    textual_features = np.array([[char_count, word_count, avg_word_length, \n",
        "                                  exclamation_count, question_count]])\n",
        "    textual_features_scaled = feature_scaler.transform(textual_features)\n",
        "    \n",
        "    # Combine features\n",
        "    from scipy.sparse import hstack\n",
        "    features = hstack([text_tfidf, textual_features_scaled])\n",
        "    \n",
        "    # Make prediction\n",
        "    prediction = model.predict(features)[0]\n",
        "    category = label_encoder.inverse_transform([prediction])[0]\n",
        "    \n",
        "    # Get prediction probability if available\n",
        "    if hasattr(model, 'predict_proba'):\n",
        "        probabilities = model.predict_proba(features)[0]\n",
        "        prob_dict = {label_encoder.classes_[i]: probabilities[i] \n",
        "                    for i in range(len(label_encoder.classes_))}\n",
        "        confidence = max(probabilities)\n",
        "    else:\n",
        "        prob_dict = None\n",
        "        confidence = None\n",
        "    \n",
        "    return {\n",
        "        'category': category,\n",
        "        'prediction': prediction,\n",
        "        'probabilities': prob_dict,\n",
        "        'confidence': confidence,\n",
        "        'original_text': article_text,\n",
        "        'cleaned_text': cleaned_text\n",
        "    }\n",
        "\n",
        "print(\"Prediction function created!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Predict Category for Sample Articles\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample articles for testing\n",
        "sample_articles = [\n",
        "    \"Scientists discover new breakthrough in renewable energy technology that could revolutionize solar power efficiency.\",\n",
        "    \"Local basketball team wins championship after thrilling overtime victory in the final game of the season.\",\n",
        "    \"New study reveals benefits of meditation and mindfulness practices for mental health and stress reduction.\",\n",
        "    \"Political leaders meet to discuss climate change policies and international cooperation agreements.\",\n",
        "    \"Tech company announces revolutionary AI system that can understand and process natural language more accurately.\"\n",
        "]\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"PREDICTING CATEGORY FOR SAMPLE ARTICLES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for i, article in enumerate(sample_articles, 1):\n",
        "    result = predict_category(article)\n",
        "    \n",
        "    print(f\"\\nArticle {i}:\")\n",
        "    print(f\"Text: {article[:100]}...\")\n",
        "    print(f\"Predicted Category: {result['category'].upper()}\")\n",
        "    if result['probabilities']:\n",
        "        # Show top 3 probabilities\n",
        "        sorted_probs = sorted(result['probabilities'].items(), key=lambda x: x[1], reverse=True)[:3]\n",
        "        print(f\"Top 3 Probabilities:\")\n",
        "        for cat, prob in sorted_probs:\n",
        "            print(f\"  {cat}: {prob:.2%}\")\n",
        "        print(f\"Confidence: {result['confidence']:.2%}\")\n",
        "    print(\"-\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Interactive Prediction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enter your own article here\n",
        "new_article = input(\"Enter a news article to analyze: \")\n",
        "\n",
        "if new_article.strip():\n",
        "    result = predict_category(new_article)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"PREDICTION RESULT\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"\\nArticle: {new_article}\")\n",
        "    print(f\"\\nPredicted Category: {result['category'].upper()}\")\n",
        "    \n",
        "    if result['probabilities']:\n",
        "        print(f\"\\nConfidence: {result['confidence']:.2%}\")\n",
        "        print(f\"\\nTop 5 Category Probabilities:\")\n",
        "        sorted_probs = sorted(result['probabilities'].items(), key=lambda x: x[1], reverse=True)[:5]\n",
        "        for category, prob in sorted_probs:\n",
        "            print(f\"  {category}: {prob:.2%}\")\n",
        "    \n",
        "    print(\"=\"*80)\n",
        "else:\n",
        "    print(\"No article entered.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Batch Prediction from File\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Predict category for multiple articles from a CSV file\n",
        "# Uncomment and modify the code below to use your own file\n",
        "\n",
        "\"\"\"\n",
        "# Load articles from CSV file\n",
        "articles_df = pd.read_csv('path/to/your/articles.csv')\n",
        "\n",
        "# Make predictions\n",
        "predictions = []\n",
        "for article in articles_df['article']:\n",
        "    result = predict_category(article)\n",
        "    predictions.append(result['category'])\n",
        "\n",
        "# Add predictions to dataframe\n",
        "articles_df['predicted_category'] = predictions\n",
        "\n",
        "# Save results\n",
        "articles_df.to_csv('predictions_results.csv', index=False)\n",
        "print(f\"Predictions saved for {len(articles_df)} articles!\")\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### Key Features:\n",
        "1. ✅ Load trained model and preprocessing components\n",
        "2. ✅ Preprocess new article text\n",
        "3. ✅ Make category predictions\n",
        "4. ✅ Get prediction probabilities and confidence scores\n",
        "5. ✅ Support for single and batch predictions\n",
        "\n",
        "### Usage Tips:\n",
        "- The model works best with articles similar to the training data\n",
        "- Longer articles generally provide better predictions\n",
        "- The confidence score indicates how certain the model is about its prediction\n",
        "- Lower confidence scores may indicate ambiguous articles\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
