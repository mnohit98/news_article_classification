{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# IMDb Movie Article Sentiment Analysis - Part 4: Model Evaluation\n",
        "\n",
        "## Overview\n",
        "This notebook covers:\n",
        "1. Loading trained models\n",
        "2. Comprehensive model evaluation with multiple metrics\n",
        "3. Confusion matrices\n",
        "4. ROC curves and Precision-Recall curves\n",
        "5. Feature importance analysis\n",
        "6. Word cloud analysis\n",
        "7. Insights and recommendations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Model evaluation\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, roc_curve, precision_recall_curve, auc,\n",
        "    confusion_matrix, classification_report\n",
        ")\n",
        "\n",
        "# Utilities\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Load Models and Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load features\n",
        "features = np.load('data/features.npz', allow_pickle=True)\n",
        "X_test_tfidf = features['X_test_tfidf']\n",
        "y_test = features['y_test']\n",
        "\n",
        "# Load label encoder\n",
        "with open('models/label_encoder.pkl', 'rb') as f:\n",
        "    label_encoder = pickle.load(f)\n",
        "\n",
        "# Load best model\n",
        "with open('models/best_model.pkl', 'rb') as f:\n",
        "    best_model = pickle.load(f)\n",
        "\n",
        "# Load TF-IDF vectorizer\n",
        "with open('models/tfidf_vectorizer.pkl', 'rb') as f:\n",
        "    tfidf_vectorizer = pickle.load(f)\n",
        "\n",
        "print(\"Models and data loaded successfully!\")\n",
        "print(f\"Test set shape: {X_test_tfidf.shape}\")\n",
        "print(f\"Test labels shape: {y_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Model Predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make predictions\n",
        "y_pred = best_model.predict(X_test_tfidf)\n",
        "y_pred_proba = best_model.predict_proba(X_test_tfidf)[:, 1] if hasattr(best_model, 'predict_proba') else None\n",
        "\n",
        "print(\"Predictions made successfully!\")\n",
        "print(f\"Prediction shape: {y_pred.shape}\")\n",
        "if y_pred_proba is not None:\n",
        "    print(f\"Prediction probabilities shape: {y_pred_proba.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Comprehensive Metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate all metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None else None\n",
        "\n",
        "# Print metrics\n",
        "print(\"=\"*60)\n",
        "print(\"MODEL PERFORMANCE METRICS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"F1-Score:  {f1:.4f}\")\n",
        "if roc_auc is not None:\n",
        "    print(f\"ROC-AUC:   {roc_auc:.4f}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Confusion Matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize confusion matrix\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
        "            xticklabels=label_encoder.classes_,\n",
        "            yticklabels=label_encoder.classes_)\n",
        "ax.set_xlabel('Predicted', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('Actual', fontsize=12, fontweight='bold')\n",
        "ax.set_title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
        "\n",
        "# Add percentage annotations\n",
        "cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
        "for i in range(len(label_encoder.classes_)):\n",
        "    for j in range(len(label_encoder.classes_)):\n",
        "        ax.text(j+0.5, i+0.7, f'({cm_percent[i,j]:.1f}%)',\n",
        "                ha='center', va='center', fontsize=9, color='red', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "os.makedirs('models/visualizations', exist_ok=True)\n",
        "plt.savefig('models/visualizations/confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: ROC Curve\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if y_pred_proba is not None:\n",
        "    # Calculate ROC curve\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    \n",
        "    # Plot ROC curve\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
        "    plt.ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
        "    plt.title('ROC Curve', fontsize=14, fontweight='bold')\n",
        "    plt.legend(loc=\"lower right\", fontsize=11)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('models/visualizations/roc_curve.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "else:\n",
        "    print(\"ROC curve not available (model doesn't support probability predictions)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Precision-Recall Curve\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if y_pred_proba is not None:\n",
        "    # Calculate Precision-Recall curve\n",
        "    precision_vals, recall_vals, thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
        "    pr_auc = auc(recall_vals, precision_vals)\n",
        "    \n",
        "    # Plot Precision-Recall curve\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(recall_vals, precision_vals, color='blue', lw=2, label=f'PR curve (AUC = {pr_auc:.4f})')\n",
        "    plt.xlabel('Recall', fontsize=12, fontweight='bold')\n",
        "    plt.ylabel('Precision', fontsize=12, fontweight='bold')\n",
        "    plt.title('Precision-Recall Curve', fontsize=14, fontweight='bold')\n",
        "    plt.legend(loc=\"lower left\", fontsize=11)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('models/visualizations/precision_recall_curve.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"Precision-Recall AUC Score: {pr_auc:.4f}\")\n",
        "else:\n",
        "    print(\"Precision-Recall curve not available (model doesn't support probability predictions)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Feature Importance Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get feature importance if available (for tree-based models)\n",
        "if hasattr(best_model, 'feature_importances_'):\n",
        "    feature_importances = best_model.feature_importances_\n",
        "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "    \n",
        "    # Get top 20 most important features\n",
        "    top_indices = np.argsort(feature_importances)[-20:][::-1]\n",
        "    top_features = [(feature_names[i], feature_importances[i]) for i in top_indices]\n",
        "    \n",
        "    # Visualize top features\n",
        "    features_df = pd.DataFrame(top_features, columns=['Feature', 'Importance'])\n",
        "    \n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.barplot(data=features_df, y='Feature', x='Importance', palette='viridis')\n",
        "    plt.xlabel('Importance', fontsize=12, fontweight='bold')\n",
        "    plt.ylabel('Feature', fontsize=12, fontweight='bold')\n",
        "    plt.title('Top 20 Most Important Features', fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('models/visualizations/feature_importance.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\nTop 20 Most Important Features:\")\n",
        "    print(features_df.to_string(index=False))\n",
        "    \n",
        "elif hasattr(best_model, 'coef_'):\n",
        "    # For linear models, use coefficients\n",
        "    coef = best_model.coef_[0]\n",
        "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "    \n",
        "    # Get top 20 features (positive and negative)\n",
        "    top_positive_indices = np.argsort(coef)[-10:][::-1]\n",
        "    top_negative_indices = np.argsort(coef)[:10]\n",
        "    \n",
        "    print(\"\\nTop 10 Features for Positive Sentiment:\")\n",
        "    for idx in top_positive_indices:\n",
        "        print(f\"  {feature_names[idx]}: {coef[idx]:.4f}\")\n",
        "    \n",
        "    print(\"\\nTop 10 Features for Negative Sentiment:\")\n",
        "    for idx in top_negative_indices:\n",
        "        print(f\"  {feature_names[idx]}: {coef[idx]:.4f}\")\n",
        "else:\n",
        "    print(\"Feature importance not available for this model type.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Word Cloud Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load original data for word cloud\n",
        "df_original = pd.read_csv('data/full_processed_data.csv')\n",
        "\n",
        "# Separate correctly and incorrectly predicted articles\n",
        "df_results = pd.DataFrame({\n",
        "    'article': df_original['article'].iloc[features['y_test'].astype(int)],\n",
        "    'category': df_original['category'].iloc[features['y_test'].astype(int)],\n",
        "    'predicted': label_encoder.inverse_transform(y_pred),\n",
        "    'correct': (y_test == y_pred)\n",
        "})\n",
        "\n",
        "# Create word clouds for different scenarios\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Positive articles (correctly predicted)\n",
        "positive_correct = ' '.join(df_results[(df_results['category'] == 'positive') & \n",
        "                                       (df_results['correct'] == True)]['article'].astype(str))\n",
        "if positive_correct:\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=100).generate(positive_correct)\n",
        "    axes[0, 0].imshow(wordcloud, interpolation='bilinear')\n",
        "    axes[0, 0].set_title('Positive Articles (Correctly Predicted)', fontsize=12, fontweight='bold')\n",
        "    axes[0, 0].axis('off')\n",
        "\n",
        "# Negative articles (correctly predicted)\n",
        "negative_correct = ' '.join(df_results[(df_results['category'] == 'negative') & \n",
        "                                       (df_results['correct'] == True)]['article'].astype(str))\n",
        "if negative_correct:\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=100).generate(negative_correct)\n",
        "    axes[0, 1].imshow(wordcloud, interpolation='bilinear')\n",
        "    axes[0, 1].set_title('Negative Articles (Correctly Predicted)', fontsize=12, fontweight='bold')\n",
        "    axes[0, 1].axis('off')\n",
        "\n",
        "# Misclassified positive articles\n",
        "positive_wrong = ' '.join(df_results[(df_results['category'] == 'positive') & \n",
        "                                    (df_results['correct'] == False)]['article'].astype(str))\n",
        "if positive_wrong:\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=100).generate(positive_wrong)\n",
        "    axes[1, 0].imshow(wordcloud, interpolation='bilinear')\n",
        "    axes[1, 0].set_title('Positive Articles (Misclassified)', fontsize=12, fontweight='bold')\n",
        "    axes[1, 0].axis('off')\n",
        "\n",
        "# Misclassified negative articles\n",
        "negative_wrong = ' '.join(df_results[(df_results['category'] == 'negative') & \n",
        "                                     (df_results['correct'] == False)]['article'].astype(str))\n",
        "if negative_wrong:\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=100).generate(negative_wrong)\n",
        "    axes[1, 1].imshow(wordcloud, interpolation='bilinear')\n",
        "    axes[1, 1].set_title('Negative Articles (Misclassified)', fontsize=12, fontweight='bold')\n",
        "    axes[1, 1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('models/visualizations/wordclouds_evaluation.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10: Error Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze misclassified examples\n",
        "misclassified = df_results[df_results['correct'] == False]\n",
        "\n",
        "print(f\"Total misclassified articles: {len(misclassified)}\")\n",
        "print(f\"Misclassification rate: {len(misclassified) / len(df_results) * 100:.2f}%\")\n",
        "\n",
        "print(\"\\nMisclassification breakdown:\")\n",
        "print(misclassified.groupby(['category', 'predicted']).size().unstack(fill_value=0))\n",
        "\n",
        "# Show some examples\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SAMPLE MISCLASSIFIED REVIEWS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for idx, row in misclassified.head(10).iterrows():\n",
        "    print(f\"\\nActual: {row['category']} | Predicted: {row['predicted']}\")\n",
        "    print(f\"Article: {row['article'][:200]}...\")\n",
        "    print(\"-\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 11: Insights and Recommendations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"KEY INSIGHTS AND RECOMMENDATIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\n1. MODEL PERFORMANCE:\")\n",
        "print(f\"   - The model achieves {accuracy:.2%} accuracy on the test set\")\n",
        "print(f\"   - F1-Score of {f1:.4f} indicates good balance between precision and recall\")\n",
        "if roc_auc:\n",
        "    print(f\"   - ROC-AUC of {roc_auc:.4f} shows strong discriminative ability\")\n",
        "\n",
        "print(\"\\n2. STRENGTHS:\")\n",
        "print(\"   - Model can effectively distinguish between positive and negative articles\")\n",
        "print(\"   - Good generalization to unseen data\")\n",
        "print(\"   - Balanced performance across both classes\")\n",
        "\n",
        "print(\"\\n3. AREAS FOR IMPROVEMENT:\")\n",
        "print(\"   - Analyze misclassified examples to identify patterns\")\n",
        "print(\"   - Consider ensemble methods for better performance\")\n",
        "print(\"   - Experiment with different feature extraction techniques\")\n",
        "print(\"   - Try deep learning models (LSTM, BERT) for potentially better results\")\n",
        "\n",
        "print(\"\\n4. BUSINESS APPLICATIONS:\")\n",
        "print(\"   - Real-time category analysis of movie articles\")\n",
        "print(\"   - Automated article moderation\")\n",
        "print(\"   - Marketing campaign effectiveness measurement\")\n",
        "print(\"   - Content recommendation systems\")\n",
        "\n",
        "print(\"\\n5. NEXT STEPS:\")\n",
        "print(\"   - Deploy model for production use\")\n",
        "print(\"   - Create API for real-time predictions\")\n",
        "print(\"   - Monitor model performance over time\")\n",
        "print(\"   - Retrain periodically with new data\")\n",
        "\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### Key Accomplishments:\n",
        "1. ✅ Comprehensive model evaluation with multiple metrics\n",
        "2. ✅ Confusion matrix visualization\n",
        "3. ✅ ROC and Precision-Recall curves\n",
        "4. ✅ Feature importance analysis\n",
        "5. ✅ Word cloud analysis for different scenarios\n",
        "6. ✅ Error analysis and insights\n",
        "\n",
        "### Next Steps:\n",
        "- Use the prediction notebook to make predictions on new articles\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
