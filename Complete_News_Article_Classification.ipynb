{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# News Article Classification - Complete Project\n",
        "\n",
        "## Complete End-to-End Machine Learning Pipeline\n",
        "\n",
        "This notebook contains the complete project in a single file:\n",
        "1. Data Loading and Exploration\n",
        "2. Data Preprocessing\n",
        "3. Feature Engineering\n",
        "4. Model Development and Training\n",
        "5. Model Evaluation\n",
        "6. Making Predictions on New Articles\n",
        "\n",
        "**Dataset Source:** [Google Sheets](https://docs.google.com/spreadsheets/d/1m4YMfqQxo_DcbtzGqbfZitvJmytbWUE8qjixhHmtadk/edit?gid=1552269726#gid=1552269726)\n",
        "\n",
        "**Instructions:**\n",
        "- Place your dataset as `data/news_data.csv`\n",
        "- Run all cells sequentially\n",
        "- Models and results will be saved automatically\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Setup and Data Loading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PART 1: IMPORT ALL LIBRARIES\n",
        "# ============================================================================\n",
        "\n",
        "# Data manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import time\n",
        "import re\n",
        "import string\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Text processing\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Feature extraction\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Machine Learning models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Model evaluation\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, classification_report\n",
        ")\n",
        "\n",
        "# Utilities\n",
        "from scipy.sparse import hstack\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Download NLTK data\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "except LookupError:\n",
        "    nltk.download('wordnet')\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Display settings\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"ALL LIBRARIES IMPORTED SUCCESSFULLY!\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PART 2: LOAD AND EXPLORE DATASET\n",
        "# ============================================================================\n",
        "\n",
        "df = pd.read_csv('data/news_data.csv')\n",
        "\n",
        "print(f\"Dataset loaded successfully!\")\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Columns: {list(df.columns)}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PART 3: DATA QUALITY ASSESSMENT\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"DATA QUALITY ASSESSMENT\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Check for missing values\n",
        "missing_values = df.isnull().sum()\n",
        "if missing_values.sum() > 0:\n",
        "    print(\"\\nMissing Values:\")\n",
        "    print(missing_values[missing_values > 0])\n",
        "else:\n",
        "    print(\"\\n✓ No missing values found!\")\n",
        "\n",
        "# Check for duplicates\n",
        "duplicate_count = df.duplicated().sum()\n",
        "print(f\"\\nDuplicate rows: {duplicate_count}\")\n",
        "if duplicate_count > 0:\n",
        "    df = df.drop_duplicates()\n",
        "    print(f\"✓ Removed {duplicate_count} duplicate rows\")\n",
        "\n",
        "# Normalize category names\n",
        "df['category'] = df['category'].str.strip().str.upper()\n",
        "\n",
        "# Combine headline and description\n",
        "df['headline'] = df['headline'].fillna('')\n",
        "df['short_description'] = df['short_description'].fillna('')\n",
        "df['combined_text'] = df['headline'] + ' ' + df['short_description']\n",
        "\n",
        "print(f\"\\n✓ Dataset prepared: {df.shape[0]} articles, {df['category'].nunique()} categories\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Category Distribution Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PART 4: CATEGORY DISTRIBUTION ANALYSIS\n",
        "# ============================================================================\n",
        "\n",
        "category_counts = df['category'].value_counts()\n",
        "print(\"=\"*80)\n",
        "print(\"CATEGORY DISTRIBUTION ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nTotal categories: {df['category'].nunique()}\")\n",
        "print(f\"\\nTop 10 categories:\")\n",
        "print(category_counts.head(10))\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "top_15 = category_counts.head(15)\n",
        "top_15.plot(kind='bar', ax=axes[0], color='steelblue')\n",
        "axes[0].set_title('Top 15 Categories Distribution', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Category', fontsize=12)\n",
        "axes[0].set_ylabel('Count', fontsize=12)\n",
        "axes[0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "top_10_pct = (category_counts.head(10) / len(df) * 100)\n",
        "top_10_pct.plot(kind='pie', ax=axes[1], autopct='%1.1f%%', startangle=90)\n",
        "axes[1].set_title('Top 10 Categories (Percentage)', fontsize=14, fontweight='bold')\n",
        "axes[1].set_ylabel('')\n",
        "\n",
        "plt.tight_layout()\n",
        "os.makedirs('models/visualizations', exist_ok=True)\n",
        "plt.savefig('models/visualizations/category_distribution.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Text Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PART 5: TEXT PREPROCESSING FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "# Initialize text processing components\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Clean text by removing HTML tags, special characters, and extra whitespace\"\"\"\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    text = str(text)\n",
        "    text = re.sub(r'<[^>]+>', '', text)  # Remove HTML tags\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)  # Remove URLs\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra whitespace\n",
        "    return text\n",
        "\n",
        "def preprocess_text(text, use_lemmatization=True, remove_stop=True):\n",
        "    \"\"\"Complete text preprocessing pipeline\"\"\"\n",
        "    text = clean_text(text)\n",
        "    tokens = word_tokenize(text)\n",
        "    if remove_stop:\n",
        "        tokens = [token for token in tokens if token not in stop_words]\n",
        "    if use_lemmatization:\n",
        "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "print(\"✓ Text preprocessing functions created!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PART 6: APPLY PREPROCESSING\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"APPLYING TEXT PREPROCESSING\")\n",
        "print(\"=\"*80)\n",
        "print(\"This may take a few minutes for large datasets...\")\n",
        "\n",
        "df['cleaned_text'] = df['combined_text'].apply(\n",
        "    lambda x: preprocess_text(x, use_lemmatization=True, remove_stop=True)\n",
        ")\n",
        "\n",
        "print(\"\\n✓ Text preprocessing completed!\")\n",
        "print(f\"\\nSample original: {df['combined_text'].iloc[0][:150]}...\")\n",
        "print(f\"\\nSample cleaned: {df['cleaned_text'].iloc[0][:150]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Feature Engineering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PART 7: EXTRACT TEXTUAL FEATURES\n",
        "# ============================================================================\n",
        "\n",
        "def extract_text_features(df):\n",
        "    \"\"\"Extract textual features from articles\"\"\"\n",
        "    features = df.copy()\n",
        "    features['char_count'] = features['cleaned_text'].str.len()\n",
        "    features['word_count'] = features['cleaned_text'].str.split().str.len()\n",
        "    features['avg_word_length'] = features['char_count'] / (features['word_count'] + 1)\n",
        "    features['exclamation_count'] = features['cleaned_text'].str.count('!')\n",
        "    features['question_count'] = features['cleaned_text'].str.count('?')\n",
        "    return features\n",
        "\n",
        "df_features = extract_text_features(df)\n",
        "print(\"✓ Textual features extracted!\")\n",
        "print(f\"\\nFeature statistics:\")\n",
        "print(df_features[['char_count', 'word_count', 'avg_word_length']].describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PART 8: TF-IDF VECTORIZATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"CREATING TF-IDF FEATURES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    max_features=5000,\n",
        "    ngram_range=(1, 2),\n",
        "    min_df=2,\n",
        "    max_df=0.95,\n",
        "    sublinear_tf=True\n",
        ")\n",
        "\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(df_features['cleaned_text'])\n",
        "print(f\"\\n✓ TF-IDF matrix created: {X_tfidf.shape}\")\n",
        "print(f\"  Features: {X_tfidf.shape[1]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PART 9: PREPARE TARGET VARIABLE AND COMBINE FEATURES\n",
        "# ============================================================================\n",
        "\n",
        "# Encode target variable\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(df_features['category'])\n",
        "\n",
        "print(f\"✓ Target variable encoded: {len(label_encoder.classes_)} categories\")\n",
        "print(f\"\\nCategory mapping (first 10):\")\n",
        "for i, label in enumerate(label_encoder.classes_[:10]):\n",
        "    print(f\"  {i}: {label}\")\n",
        "if len(label_encoder.classes_) > 10:\n",
        "    print(f\"  ... and {len(label_encoder.classes_) - 10} more\")\n",
        "\n",
        "# Combine TF-IDF with textual features\n",
        "textual_features = df_features[['char_count', 'word_count', 'avg_word_length', \n",
        "                               'exclamation_count', 'question_count']].values\n",
        "\n",
        "scaler = StandardScaler()\n",
        "textual_features_scaled = scaler.fit_transform(textual_features)\n",
        "\n",
        "X_combined = hstack([X_tfidf, textual_features_scaled])\n",
        "print(f\"\\n✓ Combined features: {X_combined.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PART 10: SPLIT DATA\n",
        "# ============================================================================\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_combined, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"DATA SPLIT COMPLETED\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Training set: {X_train.shape}\")\n",
        "print(f\"Test set: {X_test.shape}\")\n",
        "print(f\"Training labels: {y_train.shape}\")\n",
        "print(f\"Test labels: {y_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 5: Model Development\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PART 11: MODEL TRAINING FUNCTION\n",
        "# ============================================================================\n",
        "\n",
        "def train_and_evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
        "    \"\"\"Train a model and evaluate its performance\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training {model_name}...\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    start_time = time.time()\n",
        "    model.fit(X_train, y_train)\n",
        "    training_time = time.time() - start_time\n",
        "    \n",
        "    y_pred = model.predict(X_test)\n",
        "    \n",
        "    # Calculate metrics (multi-class)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "    \n",
        "    print(f\"\\n{model_name} Results:\")\n",
        "    print(f\"  Training Time: {training_time:.2f} seconds\")\n",
        "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"  Precision: {precision:.4f}\")\n",
        "    print(f\"  Recall: {recall:.4f}\")\n",
        "    print(f\"  F1-Score: {f1:.4f}\")\n",
        "    \n",
        "    return {\n",
        "        'model': model,\n",
        "        'model_name': model_name,\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1,\n",
        "        'training_time': training_time,\n",
        "        'y_pred': y_pred\n",
        "    }\n",
        "\n",
        "print(\"✓ Model training function created!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PART 12: TRAIN MULTIPLE MODELS\n",
        "# ============================================================================\n",
        "\n",
        "results = {}\n",
        "\n",
        "# 1. Logistic Regression\n",
        "lr_model = LogisticRegression(max_iter=1000, random_state=42, n_jobs=-1, multi_class='ovr')\n",
        "results['Logistic Regression'] = train_and_evaluate_model(\n",
        "    lr_model, X_train, X_test, y_train, y_test, \"Logistic Regression\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Naive Bayes\n",
        "nb_model = MultinomialNB(alpha=1.0)\n",
        "results['Naive Bayes'] = train_and_evaluate_model(\n",
        "    nb_model, X_train, X_test, y_train, y_test, \"Naive Bayes\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. SVM (using linear kernel for speed)\n",
        "svm_model = SVC(kernel='linear', probability=True, random_state=42)\n",
        "results['SVM'] = train_and_evaluate_model(\n",
        "    svm_model, X_train, X_test, y_train, y_test, \"SVM\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Random Forest\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, max_depth=20)\n",
        "results['Random Forest'] = train_and_evaluate_model(\n",
        "    rf_model, X_train, X_test, y_train, y_test, \"Random Forest\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. XGBoost (convert sparse to dense)\n",
        "X_train_dense = X_train.toarray() if hasattr(X_train, 'toarray') else X_train\n",
        "X_test_dense = X_test.toarray() if hasattr(X_test, 'toarray') else X_test\n",
        "\n",
        "xgb_model = XGBClassifier(random_state=42, n_jobs=-1, eval_metric='mlogloss')\n",
        "results['XGBoost'] = train_and_evaluate_model(\n",
        "    xgb_model, X_train_dense, X_test_dense, y_train, y_test, \"XGBoost\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PART 13: COMPARE MODELS\n",
        "# ============================================================================\n",
        "\n",
        "comparison_data = {\n",
        "    'Model': [],\n",
        "    'Accuracy': [],\n",
        "    'Precision': [],\n",
        "    'Recall': [],\n",
        "    'F1-Score': [],\n",
        "    'Training Time (s)': []\n",
        "}\n",
        "\n",
        "for model_name, result in results.items():\n",
        "    comparison_data['Model'].append(model_name)\n",
        "    comparison_data['Accuracy'].append(result['accuracy'])\n",
        "    comparison_data['Precision'].append(result['precision'])\n",
        "    comparison_data['Recall'].append(result['recall'])\n",
        "    comparison_data['F1-Score'].append(result['f1_score'])\n",
        "    comparison_data['Training Time (s)'].append(result['training_time'])\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "comparison_df = comparison_df.sort_values('F1-Score', ascending=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
        "for idx, metric in enumerate(metrics):\n",
        "    ax = axes[idx // 2, idx % 2]\n",
        "    comparison_df_sorted = comparison_df.sort_values(metric, ascending=True)\n",
        "    ax.barh(comparison_df_sorted['Model'], comparison_df_sorted[metric], color='steelblue')\n",
        "    ax.set_xlabel(metric, fontsize=12)\n",
        "    ax.set_title(f'{metric} Comparison', fontsize=14, fontweight='bold')\n",
        "    ax.set_xlim([0, 1])\n",
        "    for i, v in enumerate(comparison_df_sorted[metric]):\n",
        "        ax.text(v + 0.01, i, f'{v:.3f}', va='center', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('models/visualizations/model_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Select best model\n",
        "best_model_name = comparison_df.iloc[0]['Model']\n",
        "best_model = results[best_model_name]['model']\n",
        "print(f\"\\n✓ Best Model: {best_model_name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 6: Model Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PART 14: COMPREHENSIVE MODEL EVALUATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(f\"EVALUATING BEST MODEL: {best_model_name}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "y_pred = results[best_model_name]['y_pred']\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(f\"\\nAccuracy:  {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"F1-Score:  {f1:.4f}\")\n",
        "\n",
        "# Classification report\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CLASSIFICATION REPORT\")\n",
        "print(\"=\"*80)\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PART 15: CONFUSION MATRIX\n",
        "# ============================================================================\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# For multi-class, show top categories confusion matrix\n",
        "top_categories = category_counts.head(10).index.tolist()\n",
        "top_category_indices = [list(label_encoder.classes_).index(cat) for cat in top_categories if cat in label_encoder.classes_]\n",
        "\n",
        "if len(top_category_indices) > 0:\n",
        "    cm_top = cm[np.ix_(top_category_indices, top_category_indices)]\n",
        "    \n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(cm_top, annot=True, fmt='d', cmap='Blues', \n",
        "                xticklabels=[label_encoder.classes_[i] for i in top_category_indices],\n",
        "                yticklabels=[label_encoder.classes_[i] for i in top_category_indices])\n",
        "    plt.xlabel('Predicted', fontsize=12, fontweight='bold')\n",
        "    plt.ylabel('Actual', fontsize=12, fontweight='bold')\n",
        "    plt.title(f'Confusion Matrix - Top {len(top_category_indices)} Categories', fontsize=14, fontweight='bold')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('models/visualizations/confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"\\n✓ Confusion matrix saved for top {len(top_category_indices)} categories\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 7: Save Models and Make Predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PART 16: SAVE MODELS AND VECTORIZERS\n",
        "# ============================================================================\n",
        "\n",
        "os.makedirs('models', exist_ok=True)\n",
        "\n",
        "# Save best model\n",
        "with open('models/best_model.pkl', 'wb') as f:\n",
        "    pickle.dump(best_model, f)\n",
        "\n",
        "# Save TF-IDF vectorizer\n",
        "with open('models/tfidf_vectorizer.pkl', 'wb') as f:\n",
        "    pickle.dump(tfidf_vectorizer, f)\n",
        "\n",
        "# Save scaler\n",
        "with open('models/feature_scaler.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler, f)\n",
        "\n",
        "# Save label encoder\n",
        "with open('models/label_encoder.pkl', 'wb') as f:\n",
        "    pickle.dump(label_encoder, f)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"MODELS AND VECTORIZERS SAVED\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nSaved files:\")\n",
        "print(\"  - models/best_model.pkl\")\n",
        "print(\"  - models/tfidf_vectorizer.pkl\")\n",
        "print(\"  - models/feature_scaler.pkl\")\n",
        "print(\"  - models/label_encoder.pkl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PART 17: PREDICTION FUNCTION FOR NEW ARTICLES\n",
        "# ============================================================================\n",
        "\n",
        "def predict_article_category(article_text):\n",
        "    \"\"\"\n",
        "    Predict category for a new article\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    article_text : str\n",
        "        The news article text (headline + description)\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    dict : Dictionary containing prediction and probabilities\n",
        "    \"\"\"\n",
        "    # Preprocess\n",
        "    cleaned_text = preprocess_text(article_text)\n",
        "    \n",
        "    # Extract textual features\n",
        "    char_count = len(cleaned_text)\n",
        "    word_count = len(cleaned_text.split())\n",
        "    avg_word_length = char_count / (word_count + 1) if word_count > 0 else 0\n",
        "    exclamation_count = cleaned_text.count('!')\n",
        "    question_count = cleaned_text.count('?')\n",
        "    \n",
        "    # Transform using TF-IDF\n",
        "    text_tfidf = tfidf_vectorizer.transform([cleaned_text])\n",
        "    \n",
        "    # Scale textual features\n",
        "    textual_features = np.array([[char_count, word_count, avg_word_length, \n",
        "                                  exclamation_count, question_count]])\n",
        "    textual_features_scaled = scaler.transform(textual_features)\n",
        "    \n",
        "    # Combine features\n",
        "    features = hstack([text_tfidf, textual_features_scaled])\n",
        "    \n",
        "    # Make prediction\n",
        "    prediction = best_model.predict(features)[0]\n",
        "    category = label_encoder.inverse_transform([prediction])[0]\n",
        "    \n",
        "    # Get probabilities\n",
        "    if hasattr(best_model, 'predict_proba'):\n",
        "        probabilities = best_model.predict_proba(features)[0]\n",
        "        prob_dict = {label_encoder.classes_[i]: probabilities[i] \n",
        "                    for i in range(len(label_encoder.classes_))}\n",
        "        confidence = max(probabilities)\n",
        "    else:\n",
        "        prob_dict = None\n",
        "        confidence = None\n",
        "    \n",
        "    return {\n",
        "        'category': category,\n",
        "        'confidence': confidence,\n",
        "        'probabilities': prob_dict\n",
        "    }\n",
        "\n",
        "print(\"✓ Prediction function created!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PART 18: TEST PREDICTIONS ON SAMPLE ARTICLES\n",
        "# ============================================================================\n",
        "\n",
        "sample_articles = [\n",
        "    \"Scientists discover new breakthrough in renewable energy technology that could revolutionize solar power efficiency.\",\n",
        "    \"Local basketball team wins championship after thrilling overtime victory in the final game of the season.\",\n",
        "    \"New study reveals benefits of meditation and mindfulness practices for mental health and stress reduction.\",\n",
        "    \"Political leaders meet to discuss climate change policies and international cooperation agreements.\",\n",
        "    \"Tech company announces revolutionary AI system that can understand and process natural language more accurately.\"\n",
        "]\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"TESTING PREDICTIONS ON SAMPLE ARTICLES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for i, article in enumerate(sample_articles, 1):\n",
        "    result = predict_article_category(article)\n",
        "    \n",
        "    print(f\"\\nArticle {i}:\")\n",
        "    print(f\"Text: {article[:80]}...\")\n",
        "    print(f\"Predicted Category: {result['category']}\")\n",
        "    if result['probabilities']:\n",
        "        sorted_probs = sorted(result['probabilities'].items(), key=lambda x: x[1], reverse=True)[:3]\n",
        "        print(f\"Top 3 Probabilities:\")\n",
        "        for cat, prob in sorted_probs:\n",
        "            print(f\"  {cat}: {prob:.2%}\")\n",
        "        print(f\"Confidence: {result['confidence']:.2%}\")\n",
        "    print(\"-\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Insights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PART 19: PROJECT SUMMARY\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"PROJECT SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\n1. DATASET:\")\n",
        "print(f\"   - Total articles: {len(df)}\")\n",
        "print(f\"   - Categories: {df['category'].nunique()}\")\n",
        "print(f\"   - Training samples: {len(y_train)}\")\n",
        "print(f\"   - Test samples: {len(y_test)}\")\n",
        "\n",
        "print(f\"\\n2. BEST MODEL: {best_model_name}\")\n",
        "print(f\"   - Accuracy: {results[best_model_name]['accuracy']:.4f}\")\n",
        "print(f\"   - F1-Score: {results[best_model_name]['f1_score']:.4f}\")\n",
        "print(f\"   - Precision: {results[best_model_name]['precision']:.4f}\")\n",
        "print(f\"   - Recall: {results[best_model_name]['recall']:.4f}\")\n",
        "\n",
        "print(f\"\\n3. KEY INSIGHTS:\")\n",
        "print(f\"   - Model can classify articles into {len(label_encoder.classes_)} categories\")\n",
        "print(f\"   - Best performing algorithm: {best_model_name}\")\n",
        "print(f\"   - Model saved and ready for production use\")\n",
        "\n",
        "print(f\"\\n4. FILES CREATED:\")\n",
        "print(f\"   - models/best_model.pkl\")\n",
        "print(f\"   - models/tfidf_vectorizer.pkl\")\n",
        "print(f\"   - models/feature_scaler.pkl\")\n",
        "print(f\"   - models/label_encoder.pkl\")\n",
        "print(f\"   - models/visualizations/*.png\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PROJECT COMPLETED SUCCESSFULLY!\")\n",
        "print(\"=\"*80)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
