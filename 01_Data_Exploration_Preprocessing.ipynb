{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# News Article Classification - Part 1: Data Exploration & Preprocessing\n",
        "\n",
        "## Overview\n",
        "This notebook covers:\n",
        "1. Data Loading and Initial Exploration\n",
        "2. Data Quality Assessment\n",
        "3. Exploratory Data Analysis (EDA)\n",
        "4. Text Preprocessing\n",
        "5. Data Cleaning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Text processing\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "\n",
        "# Download NLTK data (run once)\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "except LookupError:\n",
        "    nltk.download('wordnet')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('taggers/averaged_perceptron_tagger')\n",
        "except LookupError:\n",
        "    nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Display settings\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "pd.set_option('display.max_colwidth', 200)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(\"NLTK data downloaded/verified!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Load Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('data/news_data.csv')\n",
        "\n",
        "print(f\"Dataset loaded successfully!\")\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Number of rows: {df.shape[0]}\")\n",
        "print(f\"Number of columns: {df.shape[1]}\")\n",
        "print(f\"Column names: {list(df.columns)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Initial Data Exploration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display first few rows\n",
        "df.head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display column information\n",
        "df.info()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "missing_values = df.isnull().sum()\n",
        "missing_percent = (missing_values / len(df)) * 100\n",
        "\n",
        "missing_df = pd.DataFrame({\n",
        "    'Column': missing_values.index,\n",
        "    'Missing Count': missing_values.values,\n",
        "    'Missing Percentage': missing_percent.values\n",
        "})\n",
        "\n",
        "missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
        "\n",
        "if len(missing_df) > 0:\n",
        "    print(\"Missing Values Found:\")\n",
        "    print(missing_df)\n",
        "else:\n",
        "    print(\"No missing values found in the dataset!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for duplicate rows\n",
        "duplicate_count = df.duplicated().sum()\n",
        "print(f\"Number of duplicate rows: {duplicate_count}\")\n",
        "\n",
        "if duplicate_count > 0:\n",
        "    print(\"Removing duplicates...\")\n",
        "    df = df.drop_duplicates()\n",
        "    print(f\"Dataset shape after removing duplicates: {df.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Target Variable Analysis (Category)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normalize category names (convert to lowercase and strip whitespace)\n",
        "df['category'] = df['category'].str.strip().str.upper()\n",
        "\n",
        "# Category distribution\n",
        "category_counts = df['category'].value_counts()\n",
        "category_percentages = df['category'].value_counts(normalize=True) * 100\n",
        "\n",
        "print(\"Category Distribution:\")\n",
        "print(category_counts)\n",
        "print(\"\\nCategory Percentages:\")\n",
        "print(category_percentages)\n",
        "print(f\"\\nTotal number of categories: {df['category'].nunique()}\")\n",
        "\n",
        "# Visualize category distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Bar chart\n",
        "top_categories = category_counts.head(15)\n",
        "top_categories.plot(kind='bar', ax=axes[0], color='steelblue')\n",
        "axes[0].set_title('Top 15 Categories Distribution (Count)', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Category', fontsize=12)\n",
        "axes[0].set_ylabel('Count', fontsize=12)\n",
        "axes[0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Pie chart for top 10 categories\n",
        "top_10_percentages = category_percentages.head(10)\n",
        "top_10_percentages.plot(kind='pie', ax=axes[1], autopct='%1.1f%%', startangle=90)\n",
        "axes[1].set_title('Top 10 Categories Distribution (Percentage)', fontsize=14, fontweight='bold')\n",
        "axes[1].set_ylabel('')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Combine Text Features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combine headline and short_description for better classification\n",
        "# Fill NaN values with empty strings\n",
        "df['headline'] = df['headline'].fillna('')\n",
        "df['short_description'] = df['short_description'].fillna('')\n",
        "\n",
        "# Combine headline and description\n",
        "df['combined_text'] = df['headline'] + ' ' + df['short_description']\n",
        "\n",
        "# Calculate text length statistics\n",
        "df['text_length'] = df['combined_text'].str.len()\n",
        "df['word_count'] = df['combined_text'].str.split().str.len()\n",
        "\n",
        "print(\"Text Length Statistics:\")\n",
        "print(df[['text_length', 'word_count']].describe())\n",
        "\n",
        "# Visualize text length distribution by category\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Character count distribution\n",
        "top_categories_list = category_counts.head(5).index.tolist()\n",
        "df_top = df[df['category'].isin(top_categories_list)]\n",
        "df_top.boxplot(column='text_length', by='category', ax=axes[0])\n",
        "axes[0].set_title('Text Length (Characters) by Category', fontsize=12, fontweight='bold')\n",
        "axes[0].set_xlabel('Category', fontsize=10)\n",
        "axes[0].set_ylabel('Character Count', fontsize=10)\n",
        "plt.setp(axes[0].xaxis.get_majorticklabels(), rotation=45)\n",
        "\n",
        "# Word count distribution\n",
        "df_top.boxplot(column='word_count', by='category', ax=axes[1])\n",
        "axes[1].set_title('Word Count by Category', fontsize=12, fontweight='bold')\n",
        "axes[1].set_xlabel('Category', fontsize=10)\n",
        "axes[1].set_ylabel('Word Count', fontsize=10)\n",
        "plt.setp(axes[1].xaxis.get_majorticklabels(), rotation=45)\n",
        "\n",
        "plt.suptitle('Text Length Analysis by Category', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Word Cloud Visualization by Category\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create word clouds for top categories\n",
        "def create_wordcloud(text, title, ax):\n",
        "    if len(text) > 0:\n",
        "        wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=100).generate(text)\n",
        "        ax.imshow(wordcloud, interpolation='bilinear')\n",
        "        ax.set_title(title, fontsize=12, fontweight='bold')\n",
        "        ax.axis('off')\n",
        "\n",
        "# Get top 4 categories\n",
        "top_4_categories = category_counts.head(4).index.tolist()\n",
        "\n",
        "# Create word clouds\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "for idx, category in enumerate(top_4_categories):\n",
        "    row = idx // 2\n",
        "    col = idx % 2\n",
        "    category_text = ' '.join(df[df['category'] == category]['combined_text'].astype(str))\n",
        "    create_wordcloud(category_text, f'{category} Articles', axes[row, col])\n",
        "\n",
        "plt.tight_layout()\n",
        "os.makedirs('models', exist_ok=True)\n",
        "plt.savefig('models/wordclouds_by_category.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Text Preprocessing Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize lemmatizer and stemmer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Clean text by removing HTML tags, special characters, and extra whitespace\"\"\"\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    \n",
        "    # Convert to string\n",
        "    text = str(text)\n",
        "    \n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<[^>]+>', '', text)\n",
        "    \n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    \n",
        "    # Remove special characters and digits (keep only letters and spaces)\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    \n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    \n",
        "    return text\n",
        "\n",
        "def tokenize_text(text):\n",
        "    \"\"\"Tokenize text into words\"\"\"\n",
        "    return word_tokenize(text)\n",
        "\n",
        "def remove_stopwords(tokens):\n",
        "    \"\"\"Remove stop words from tokens\"\"\"\n",
        "    return [token for token in tokens if token not in stop_words]\n",
        "\n",
        "def lemmatize_text(tokens):\n",
        "    \"\"\"Lemmatize tokens (convert to base form)\"\"\"\n",
        "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "def preprocess_text(text, use_lemmatization=True, remove_stop=True):\n",
        "    \"\"\"Complete text preprocessing pipeline\"\"\"\n",
        "    # Clean text\n",
        "    text = clean_text(text)\n",
        "    \n",
        "    # Tokenize\n",
        "    tokens = tokenize_text(text)\n",
        "    \n",
        "    # Remove stop words\n",
        "    if remove_stop:\n",
        "        tokens = remove_stopwords(tokens)\n",
        "    \n",
        "    # Lemmatize\n",
        "    if use_lemmatization:\n",
        "        tokens = lemmatize_text(tokens)\n",
        "    \n",
        "    # Join tokens back to string\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "print(\"Text preprocessing functions created!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Apply Text Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply preprocessing to combined text\n",
        "print(\"Starting text preprocessing...\")\n",
        "print(\"This may take a few minutes for large datasets...\")\n",
        "\n",
        "# Create a copy of the dataframe\n",
        "df_processed = df.copy()\n",
        "\n",
        "# Apply preprocessing (using lemmatization)\n",
        "df_processed['cleaned_text'] = df_processed['combined_text'].apply(\n",
        "    lambda x: preprocess_text(x, use_lemmatization=True, remove_stop=True)\n",
        ")\n",
        "\n",
        "print(\"Text preprocessing completed!\")\n",
        "print(f\"\\nSample original text:\")\n",
        "print(df_processed['combined_text'].iloc[0][:200])\n",
        "print(f\"\\nSample cleaned text:\")\n",
        "print(df_processed['cleaned_text'].iloc[0][:200])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Save Preprocessed Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save preprocessed data\n",
        "os.makedirs('data', exist_ok=True)\n",
        "\n",
        "# Save full dataframe with cleaned text\n",
        "df_processed[['cleaned_text', 'category']].to_csv('data/processed_articles.csv', index=False)\n",
        "\n",
        "# Also save original for reference\n",
        "df_processed.to_csv('data/full_processed_data.csv', index=False)\n",
        "\n",
        "print(\"Preprocessed data saved successfully!\")\n",
        "print(\"Files saved:\")\n",
        "print(\"- data/processed_articles.csv\")\n",
        "print(\"- data/full_processed_data.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### Key Findings:\n",
        "1. Dataset loaded and explored\n",
        "2. Category distribution analyzed\n",
        "3. Text length statistics calculated\n",
        "4. Text preprocessing completed (cleaning, tokenization, lemmatization)\n",
        "5. Preprocessed data saved for feature engineering\n",
        "\n",
        "### Next Steps:\n",
        "- Proceed to Feature Engineering notebook\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
