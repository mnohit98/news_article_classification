{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# IMDb Movie Article Sentiment Analysis - Part 3: Model Development\n",
        "\n",
        "## Overview\n",
        "This notebook covers:\n",
        "1. Loading features and data splits\n",
        "2. Training multiple classification models:\n",
        "   - Logistic Regression\n",
        "   - Naive Bayes\n",
        "   - Support Vector Machine (SVM)\n",
        "   - Random Forest\n",
        "   - XGBoost\n",
        "   - Neural Networks (LSTM - optional)\n",
        "3. Hyperparameter tuning\n",
        "4. Saving trained models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "import time\n",
        "\n",
        "# Machine Learning models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Model evaluation\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
        "\n",
        "# Neural Networks (optional)\n",
        "try:\n",
        "    from tensorflow import keras\n",
        "    from tensorflow.keras.models import Sequential\n",
        "    from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout\n",
        "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "    TENSORFLOW_AVAILABLE = True\n",
        "except ImportError:\n",
        "    TENSORFLOW_AVAILABLE = False\n",
        "    print(\"TensorFlow not available. Neural network models will be skipped.\")\n",
        "\n",
        "# Utilities\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Load Features and Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load features\n",
        "features = np.load('data/features.npz', allow_pickle=True)\n",
        "\n",
        "X_train_tfidf = features['X_train_tfidf']\n",
        "X_test_tfidf = features['X_test_tfidf']\n",
        "X_train_word2vec = features['X_train_word2vec']\n",
        "X_test_word2vec = features['X_test_word2vec']\n",
        "y_train = features['y_train']\n",
        "y_test = features['y_test']\n",
        "\n",
        "print(\"Features loaded successfully!\")\n",
        "print(f\"Training set (TF-IDF): {X_train_tfidf.shape}\")\n",
        "print(f\"Test set (TF-IDF): {X_test_tfidf.shape}\")\n",
        "print(f\"Training set (Word2Vec): {X_train_word2vec.shape}\")\n",
        "print(f\"Test set (Word2Vec): {X_test_word2vec.shape}\")\n",
        "print(f\"Training labels: {y_train.shape}\")\n",
        "print(f\"Test labels: {y_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Model Training Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_and_evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
        "    \"\"\"Train a model and evaluate its performance\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training {model_name}...\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train)\n",
        "    training_time = time.time() - start_time\n",
        "    \n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_proba = None\n",
        "    \n",
        "    # Get prediction probabilities if available\n",
        "    if hasattr(model, 'predict_proba'):\n",
        "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "    \n",
        "    roc_auc = None\n",
        "    if y_pred_proba is not None:\n",
        "        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "    \n",
        "    # Print results\n",
        "    print(f\"\\n{model_name} Results:\")\n",
        "    print(f\"  Training Time: {training_time:.2f} seconds\")\n",
        "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"  Precision: {precision:.4f}\")\n",
        "    print(f\"  Recall: {recall:.4f}\")\n",
        "    print(f\"  F1-Score: {f1:.4f}\")\n",
        "    if roc_auc is not None:\n",
        "        print(f\"  ROC-AUC: {roc_auc:.4f}\")\n",
        "    \n",
        "    return {\n",
        "        'model': model,\n",
        "        'model_name': model_name,\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1,\n",
        "        'roc_auc': roc_auc,\n",
        "        'training_time': training_time,\n",
        "        'y_pred': y_pred,\n",
        "        'y_pred_proba': y_pred_proba\n",
        "    }\n",
        "\n",
        "print(\"Model training function created!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Train Models with TF-IDF Features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Store results\n",
        "results_tfidf = {}\n",
        "\n",
        "# 1. Logistic Regression\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MODEL 1: Logistic Regression\")\n",
        "print(\"=\"*60)\n",
        "lr_model = LogisticRegression(max_iter=1000, random_state=42, n_jobs=-1)\n",
        "results_tfidf['Logistic Regression'] = train_and_evaluate_model(\n",
        "    lr_model, X_train_tfidf, X_test_tfidf, y_train, y_test, \"Logistic Regression\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Naive Bayes\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MODEL 2: Naive Bayes\")\n",
        "print(\"=\"*60)\n",
        "nb_model = MultinomialNB(alpha=1.0)\n",
        "results_tfidf['Naive Bayes'] = train_and_evaluate_model(\n",
        "    nb_model, X_train_tfidf, X_test_tfidf, y_train, y_test, \"Naive Bayes\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Support Vector Machine (SVM)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MODEL 3: Support Vector Machine\")\n",
        "print(\"=\"*60)\n",
        "# Note: SVM can be slow on large datasets, using linear kernel for speed\n",
        "svm_model = SVC(kernel='linear', probability=True, random_state=42)\n",
        "results_tfidf['SVM'] = train_and_evaluate_model(\n",
        "    svm_model, X_train_tfidf, X_test_tfidf, y_train, y_test, \"SVM\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Random Forest\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MODEL 4: Random Forest\")\n",
        "print(\"=\"*60)\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, max_depth=20)\n",
        "results_tfidf['Random Forest'] = train_and_evaluate_model(\n",
        "    rf_model, X_train_tfidf, X_test_tfidf, y_train, y_test, \"Random Forest\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. XGBoost\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MODEL 5: XGBoost\")\n",
        "print(\"=\"*60)\n",
        "# Convert sparse matrix to dense for XGBoost\n",
        "X_train_tfidf_dense = X_train_tfidf.toarray() if hasattr(X_train_tfidf, 'toarray') else X_train_tfidf\n",
        "X_test_tfidf_dense = X_test_tfidf.toarray() if hasattr(X_test_tfidf, 'toarray') else X_test_tfidf\n",
        "\n",
        "xgb_model = XGBClassifier(random_state=42, n_jobs=-1, eval_metric='logloss')\n",
        "results_tfidf['XGBoost'] = train_and_evaluate_model(\n",
        "    xgb_model, X_train_tfidf_dense, X_test_tfidf_dense, y_train, y_test, \"XGBoost\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Compare Model Performance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comparison dataframe\n",
        "comparison_data = {\n",
        "    'Model': [],\n",
        "    'Accuracy': [],\n",
        "    'Precision': [],\n",
        "    'Recall': [],\n",
        "    'F1-Score': [],\n",
        "    'ROC-AUC': [],\n",
        "    'Training Time (s)': []\n",
        "}\n",
        "\n",
        "for model_name, result in results_tfidf.items():\n",
        "    comparison_data['Model'].append(model_name)\n",
        "    comparison_data['Accuracy'].append(result['accuracy'])\n",
        "    comparison_data['Precision'].append(result['precision'])\n",
        "    comparison_data['Recall'].append(result['recall'])\n",
        "    comparison_data['F1-Score'].append(result['f1_score'])\n",
        "    comparison_data['ROC-AUC'].append(result['roc_auc'] if result['roc_auc'] is not None else np.nan)\n",
        "    comparison_data['Training Time (s)'].append(result['training_time'])\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "comparison_df = comparison_df.sort_values('F1-Score', ascending=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL COMPARISON (TF-IDF Features)\")\n",
        "print(\"=\"*80)\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "# Visualize comparison\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
        "for idx, metric in enumerate(metrics):\n",
        "    ax = axes[idx // 2, idx % 2]\n",
        "    comparison_df_sorted = comparison_df.sort_values(metric, ascending=True)\n",
        "    ax.barh(comparison_df_sorted['Model'], comparison_df_sorted[metric], color='steelblue')\n",
        "    ax.set_xlabel(metric, fontsize=12)\n",
        "    ax.set_title(f'{metric} Comparison', fontsize=14, fontweight='bold')\n",
        "    ax.set_xlim([0, 1])\n",
        "    for i, v in enumerate(comparison_df_sorted[metric]):\n",
        "        ax.text(v + 0.01, i, f'{v:.3f}', va='center', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "os.makedirs('models', exist_ok=True)\n",
        "plt.savefig('models/model_comparison_tfidf.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Hyperparameter Tuning (Best Model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find best model\n",
        "best_model_name = comparison_df.iloc[0]['Model']\n",
        "print(f\"\\nBest performing model: {best_model_name}\")\n",
        "\n",
        "# Hyperparameter tuning for best model\n",
        "print(f\"\\nPerforming hyperparameter tuning for {best_model_name}...\")\n",
        "\n",
        "if best_model_name == 'Logistic Regression':\n",
        "    param_grid = {\n",
        "        'C': [0.1, 1, 10, 100],\n",
        "        'penalty': ['l1', 'l2'],\n",
        "        'solver': ['liblinear']\n",
        "    }\n",
        "    base_model = LogisticRegression(max_iter=1000, random_state=42, n_jobs=-1)\n",
        "    \n",
        "elif best_model_name == 'Naive Bayes':\n",
        "    param_grid = {\n",
        "        'alpha': [0.1, 0.5, 1.0, 2.0]\n",
        "    }\n",
        "    base_model = MultinomialNB()\n",
        "    \n",
        "elif best_model_name == 'SVM':\n",
        "    param_grid = {\n",
        "        'C': [0.1, 1, 10],\n",
        "        'kernel': ['linear', 'rbf']\n",
        "    }\n",
        "    base_model = SVC(probability=True, random_state=42)\n",
        "    \n",
        "elif best_model_name == 'Random Forest':\n",
        "    param_grid = {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_depth': [10, 20, None],\n",
        "        'min_samples_split': [2, 5]\n",
        "    }\n",
        "    base_model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
        "    \n",
        "elif best_model_name == 'XGBoost':\n",
        "    param_grid = {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'learning_rate': [0.01, 0.1, 0.2]\n",
        "    }\n",
        "    base_model = XGBClassifier(random_state=42, n_jobs=-1, eval_metric='logloss')\n",
        "    X_train_tuned = X_train_tfidf_dense\n",
        "    X_test_tuned = X_test_tfidf_dense\n",
        "else:\n",
        "    X_train_tuned = X_train_tfidf\n",
        "    X_test_tuned = X_test_tfidf\n",
        "\n",
        "if best_model_name != 'XGBoost':\n",
        "    X_train_tuned = X_train_tfidf\n",
        "    X_test_tuned = X_test_tfidf\n",
        "\n",
        "# Perform grid search\n",
        "print(\"Running GridSearchCV (this may take a while)...\")\n",
        "grid_search = GridSearchCV(\n",
        "    base_model, \n",
        "    param_grid, \n",
        "    cv=3, \n",
        "    scoring='f1_weighted',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train_tuned, y_train)\n",
        "\n",
        "print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best CV score: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Evaluate tuned model\n",
        "best_tuned_model = grid_search.best_estimator_\n",
        "y_pred_tuned = best_tuned_model.predict(X_test_tuned)\n",
        "\n",
        "print(f\"\\nTuned {best_model_name} Performance:\")\n",
        "print(f\"  Accuracy: {accuracy_score(y_test, y_pred_tuned):.4f}\")\n",
        "print(f\"  F1-Score: {f1_score(y_test, y_pred_tuned, average='weighted'):.4f}\")\n",
        "\n",
        "# Update results\n",
        "results_tfidf[f'{best_model_name} (Tuned)'] = {\n",
        "    'model': best_tuned_model,\n",
        "    'model_name': f'{best_model_name} (Tuned)',\n",
        "    'accuracy': accuracy_score(y_test, y_pred_tuned),\n",
        "    'precision': precision_score(y_test, y_pred_tuned, average='weighted'),\n",
        "    'recall': recall_score(y_test, y_pred_tuned, average='weighted'),\n",
        "    'f1_score': f1_score(y_test, y_pred_tuned, average='weighted'),\n",
        "    'roc_auc': roc_auc_score(y_test, best_tuned_model.predict_proba(X_test_tuned)[:, 1]) if hasattr(best_tuned_model, 'predict_proba') else None,\n",
        "    'training_time': 0,\n",
        "    'y_pred': y_pred_tuned,\n",
        "    'y_pred_proba': best_tuned_model.predict_proba(X_test_tuned)[:, 1] if hasattr(best_tuned_model, 'predict_proba') else None\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save all models\n",
        "os.makedirs('models', exist_ok=True)\n",
        "\n",
        "for model_name, result in results_tfidf.items():\n",
        "    # Clean model name for filename\n",
        "    filename = model_name.lower().replace(' ', '_').replace('(', '').replace(')', '')\n",
        "    filepath = f'models/{filename}.pkl'\n",
        "    \n",
        "    with open(filepath, 'wb') as f:\n",
        "        pickle.dump(result['model'], f)\n",
        "    \n",
        "    print(f\"Saved: {filepath}\")\n",
        "\n",
        "# Save best model separately\n",
        "best_model_result = results_tfidf[best_model_name]\n",
        "with open('models/best_model.pkl', 'wb') as f:\n",
        "    pickle.dump(best_model_result['model'], f)\n",
        "\n",
        "print(f\"\\nBest model saved: models/best_model.pkl ({best_model_name})\")\n",
        "print(\"\\nAll models saved successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### Key Accomplishments:\n",
        "1. ✅ Trained 5 different classification models\n",
        "2. ✅ Compared model performance across multiple metrics\n",
        "3. ✅ Performed hyperparameter tuning on best model\n",
        "4. ✅ Saved all trained models for evaluation\n",
        "\n",
        "### Next Steps:\n",
        "- Proceed to Model Evaluation notebook for detailed analysis\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
